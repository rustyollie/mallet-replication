{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cce273c2-dac3-4855-9955-2e6311d31164",
   "metadata": {},
   "source": [
    "# HATHITRUST VOLUME DEDUPLICATION PIPELINE\n",
    "# ==========================================\n",
    "#\n",
    "# This notebook deduplicates a HathiTrust workset of volumes printed in England (1500-1900),\n",
    "# written in English only. For serial publications (multi-volume works), it  \n",
    "# selects the most complete set from a single university, then fills gaps with volumes from \n",
    "# other universities.\n",
    "#\n",
    "# PIPELINE OVERVIEW:\n",
    "# 1. Load and filter workset (year, language)\n",
    "# 2. Separate unique vs duplicated volumes\n",
    "# 3. For duplicates: separate serials vs non-serials\n",
    "# 4. Standardize serial volume descriptions (v.1, vol.1, V1 → v.1)\n",
    "# 5. Select most complete serial set per university\n",
    "# 6. Fill missing volumes from other universities\n",
    "# 7. Export final deduplicated list for download via HTRC Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Load HTRC workset\n",
    "# Load the initial workset CSV file containing all volumes printed in England (1500-1900)\n",
    "# as provided by the HTRC Librarian\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "initial_workset = pd.read_csv('./htrc_workset_final.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f781925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Filter workset by year and language\n",
    "# Keep only volumes that:\n",
    "#   - Have a valid year field\n",
    "#   - Were published between 1500-1900\n",
    "#   - Are in English only (language='eng')\n",
    "\n",
    "filtered_workset = initial_workset[~initial_workset['year'].isna()]\n",
    "filtered_workset = filtered_workset[(filtered_workset['year'] <= 1900) & (filtered_workset['year'] >= 1500)]\n",
    "filtered_workset = filtered_workset[filtered_workset['language'] == 'eng']\n",
    "filtered_workset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca28c81-eb3a-4504-887b-66e672e2dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Separate unique vs duplicated volumes\n",
    "# Split volumes into two groups based on record_id:\n",
    "#   - unique_volumes: Only one copy exists across all universities (keep as-is)\n",
    "#   - duplicated_volumes: Multiple copies exist (need deduplication)\n",
    "\n",
    "unique_volumes = filtered_workset[filtered_workset.duplicated(subset=['record_id'], keep=False) == False]\n",
    "duplicated_volumes = filtered_workset[filtered_workset.duplicated(subset=['record_id'], keep=False) == True]\n",
    "duplicated_htids = set(duplicated_volumes.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c56156-4f54-4764-9b35-eec8ce442d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Load HathiTrust metadata for duplicated volumes \n",
    "# Load metadata in chunks with filtering to minimize memory usage\n",
    "# Filter each chunk to only keep duplicated volumes\n",
    "\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from tqdm import tqdm\n",
    "\n",
    "def parse_metadata_line(line):\n",
    "    \"\"\"Handle metadata lines with too many fields by combining extras into last field\"\"\"\n",
    "    fields = line.strip().split('\\t')\n",
    "    if len(fields) > 26:\n",
    "        fields = fields[:25] + ['\\t'.join(fields[25:])]\n",
    "    return fields[:26]\n",
    "\n",
    "# Column names for HathiTrust metadata\n",
    "METADATA_COLUMNS = [\n",
    "    'htid', 'access', 'rights', 'ht_bib_key', 'description',\n",
    "    'source', 'source_bib_num', 'oclc_num', 'isbn', 'issn',\n",
    "    'lccn', 'title', 'imprint', 'rights_reason_code',\n",
    "    'rights_timestamp', 'us_gov_doc_flag', 'rights_date_used',\n",
    "    'pub_place', 'lang', 'bib_fmt', 'collection_code',\n",
    "    'content_provider_code', 'responsible_entity_code',\n",
    "    'digitization_agent_code', 'access_profile_code',\n",
    "    'author'\n",
    "]\n",
    "\n",
    "# Read file in chunks, filtering as we go\n",
    "chunk_size = 100000\n",
    "filtered_chunks = []\n",
    "chunk_lines = []\n",
    "\n",
    "with gzip.open(r'.\\hathi_full_20241001.txt.gz', 'rt', encoding='utf-8') as file:\n",
    "    # Process file with progress bar (estimated ~17M lines)\n",
    "    for line in tqdm(file, desc=\"Loading metadata\", unit=\" lines\", total=17000000):\n",
    "        parsed = parse_metadata_line(line)\n",
    "        \n",
    "        # Only keep lines for duplicated volumes\n",
    "        if parsed[0] in duplicated_htids:\n",
    "            chunk_lines.append(parsed)\n",
    "        \n",
    "        # Process chunk when it reaches size limit\n",
    "        if len(chunk_lines) >= chunk_size:\n",
    "            chunk_df = pd.DataFrame(chunk_lines, columns=METADATA_COLUMNS)\n",
    "            filtered_chunks.append(chunk_df)\n",
    "            chunk_lines = []\n",
    "    \n",
    "    # Process remaining lines\n",
    "    if chunk_lines:\n",
    "        chunk_df = pd.DataFrame(chunk_lines, columns=METADATA_COLUMNS)\n",
    "        filtered_chunks.append(chunk_df)\n",
    "\n",
    "# Combine all filtered chunks\n",
    "hathi_metadata = pd.concat(filtered_chunks, ignore_index=True)\n",
    "del filtered_chunks, chunk_lines\n",
    "\n",
    "print(f\"Loaded {len(hathi_metadata):,} metadata records for duplicated volumes\")\n",
    "hathi_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0bf86-8b32-43a5-b02c-df173978e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Separate non-serial volumes\n",
    "# Volumes with empty 'description' field are NOT part of a serial publication\n",
    "# For these, any copy is equivalent - keep first occurrence per record_id\n",
    "\n",
    "nonserial_volumes = hathi_metadata[hathi_metadata['description'] =='']\n",
    "nonserial_volumes = nonserial_volumes.drop_duplicates(subset = ['ht_bib_key'], keep = 'first')\n",
    "nonserial_volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ce660-df72-419b-96a8-0c8fec69b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Identify serial volumes\n",
    "# Volumes with non-empty 'description' field are part of a serial publication\n",
    "# Count occurrences of each description to understand the data before standardization\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "serial_volumes = hathi_metadata[hathi_metadata['description'] !='']\n",
    "description_counts = Counter(serial_volumes['description'])\n",
    "volume_descriptions = pd.DataFrame.from_dict(description_counts, orient='index', columns=['count']).reset_index()\n",
    "volume_descriptions = volume_descriptions.sort_values('count', ascending=False)\n",
    "volume_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9656ec1-52b6-415f-a330-93b99f671135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: Standardize volume descriptions\n",
    "# Serial volume descriptions vary widely (v.1, vol.1, V1, v. 1, v.001, etc.)\n",
    "# Apply series of regex transformations to normalize all formats to \"v.N\"\n",
    "# \n",
    "# Transformations applied in order:\n",
    "#   1. vol → v  (e.g., vol1 → v1)\n",
    "#   2. V → v   (e.g., V1 → v1)\n",
    "#   3. v<space>N → v.N  (e.g., v 1 → v.1)\n",
    "#   4. v.<space>N → v.N  (e.g., v. 1 → v.1)\n",
    "#   5. Remove leading zeros  (e.g., v.001 → v.1)\n",
    "\n",
    "import re\n",
    "\n",
    "def vol_v(s):\n",
    "    \"\"\"Convert 'vol' to 'v', handle single numeric strings\"\"\"\n",
    "    if 'vol' in s.lower():\n",
    "        return s.lower().replace('vol','v')\n",
    "    if len(s) < 2 and s.isnumeric():\n",
    "        return 'v.' + s\n",
    "    return s\n",
    "\n",
    "def capital_v(s):\n",
    "    \"\"\"Convert capital 'V' to lowercase 'v'\"\"\"\n",
    "    pattern = r'V([0-9\\W]*)$'\n",
    "    if re.search(pattern, s):\n",
    "        return re.sub(pattern, r'v\\1', s)\n",
    "    return s\n",
    "\n",
    "def process_v_number(s):\n",
    "    \"\"\"Convert 'v' followed by space and number to 'v.number'\"\"\"\n",
    "    pattern = r'(?:^|\\s)([^\\s]*\\s)?v\\s*(\\d+)'\n",
    "    match = re.search(pattern, s, re.IGNORECASE)\n",
    "    if match:\n",
    "        return f'v.{match.group(2)}'\n",
    "    return s\n",
    "\n",
    "def process_v_dot(s):\n",
    "    \"\"\"Remove space between 'v.' and number\"\"\"\n",
    "    pattern = r'\\bv\\.\\s*((?:\\d+-?)+\\d*)'\n",
    "    match = re.search(pattern, s)\n",
    "    if match:\n",
    "        return f'v.{match.group(1)}'\n",
    "    return s\n",
    "\n",
    "def remove_leading_zeros(s):\n",
    "    \"\"\"Remove leading zeros from volume numbers\"\"\"\n",
    "    pattern = r'(v\\.)0*(\\d+)'\n",
    "    def replace_zeros(match):\n",
    "        prefix = match.group(1)\n",
    "        number = match.group(2)\n",
    "        return f\"{prefix}{number}\"\n",
    "    return re.sub(pattern, replace_zeros, s)\n",
    "\n",
    "# Apply transformations sequentially\n",
    "volume_descriptions['step1_vol_to_v'] = volume_descriptions['index'].apply(vol_v)\n",
    "volume_descriptions['step2_lowercase_v'] = volume_descriptions['step1_vol_to_v'].apply(capital_v)\n",
    "volume_descriptions['step3_add_dot'] = volume_descriptions['step2_lowercase_v'].apply(process_v_number)\n",
    "volume_descriptions['step4_remove_space'] = volume_descriptions['step3_add_dot'].apply(process_v_dot)\n",
    "volume_descriptions['standardized_description'] = volume_descriptions['step4_remove_space'].apply(remove_leading_zeros)\n",
    "\n",
    "standardized_descriptions = volume_descriptions[['index','standardized_description']]\n",
    "standardized_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8204ca-656b-48d1-ae0b-8dd8602488bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 8: Merge standardized descriptions with serial volumes\n",
    "# Join serial volumes with their standardized descriptions\n",
    "# Remove duplicates based on: record_id + standardized volume + source university\n",
    "# This ensures each volume number appears only once per source\n",
    "\n",
    "serials_with_std_desc = pd.merge(serial_volumes, standardized_descriptions, left_on='description', right_on='index')\n",
    "serials_with_std_desc = serials_with_std_desc.drop_duplicates(subset = ['ht_bib_key', 'standardized_description','source'], keep = 'first')\n",
    "serial_record_ids = set(serials_with_std_desc['ht_bib_key'])\n",
    "serials_with_std_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ecbcd8-916a-4554-8655-5b53aa80ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 9: Count volumes per source for each serial\n",
    "# For each record_id, count how many volumes each university has\n",
    "# This identifies which university has the most complete set\n",
    "\n",
    "volumes_per_source = serials_with_std_desc[['ht_bib_key','source','standardized_description']]\n",
    "volumes_per_source[['count']] = 1\n",
    "volumes_per_source = volumes_per_source.groupby(['ht_bib_key','source']).sum()\n",
    "volumes_per_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff976-c800-45e1-a9ac-e2b3331fc55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 10: Select university with most complete serial set\n",
    "# For each record_id, find which university (source) has the highest volume count\n",
    "# In case of ties, idxmax() returns the first alphabetically\n",
    "\n",
    "best_sources_list = []\n",
    "for rec_id in serial_record_ids:\n",
    "    subset = volumes_per_source.loc[rec_id,:]\n",
    "    best_sources_list.append((rec_id, subset['count'].idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c41ed5-2e12-4178-b48a-966a8eefc7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 11: Extract volumes from top sources\n",
    "# Keep only volumes from the university with the most complete set for each serial\n",
    "\n",
    "best_sources = pd.DataFrame(best_sources_list, columns=['ht_bib_key', 'source'])\n",
    "serials_from_best_source = serials_with_std_desc.merge(best_sources, on=['ht_bib_key', 'source'], how='inner')\n",
    "deduplicated_serials = serials_from_best_source[['htid', 'ht_bib_key', 'source', 'description','standardized_description', 'rights_date_used']]\n",
    "deduplicated_serials = deduplicated_serials.reset_index(drop=True)\n",
    "deduplicated_serials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b3f3d9-b739-46e1-85ba-72c05f73c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 12: Identify all publication years for each serial across ALL sources\n",
    "# Group by record_id and collect all unique publication years (rights_date_used)\n",
    "# This shows the complete year range across all universities\n",
    "\n",
    "all_years_all_sources = serials_with_std_desc\n",
    "all_years_all_sources = all_years_all_sources[['ht_bib_key', 'rights_date_used']].groupby('ht_bib_key').agg(set)\n",
    "all_years_all_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad5dfa-d744-48ef-ad72-f82a92e0c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 13: Find missing volumes in top source\n",
    "# Compare years from top source vs all sources to find gaps\n",
    "# These missing volumes can be filled from other universities\n",
    "\n",
    "def set_difference(row):\n",
    "    \"\"\"Return publication years in full set but not in selected source\"\"\"\n",
    "    return list(set(row['rights_date_used_all']) - set(row['rights_date_used_best']))\n",
    "    \n",
    "missing_years_in_best_source = deduplicated_serials\n",
    "missing_years_in_best_source = missing_years_in_best_source[['ht_bib_key', 'rights_date_used']].groupby('ht_bib_key').agg(set)\n",
    "missing_years_in_best_source = missing_years_in_best_source.merge(all_years_all_sources, on=['ht_bib_key'], how='inner', suffixes = ('_best', '_all'))\n",
    "missing_years_in_best_source['set_diff'] = missing_years_in_best_source.apply(set_difference, axis=1)\n",
    "missing_years_in_best_source = missing_years_in_best_source[missing_years_in_best_source['set_diff'].apply(lambda x: len(x) != 0)]\n",
    "missing_years_in_best_source = missing_years_in_best_source[['set_diff']].explode('set_diff').reset_index()\n",
    "missing_years_in_best_source = missing_years_in_best_source.rename(columns={'set_diff':'rights_date_used'})\n",
    "missing_years_in_best_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ad562d-8461-43d2-a9b3-8bb58226843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 14: Add missing volumes from other sources\n",
    "# For each missing year, find the volume from any other university\n",
    "# Add these to complete the serial set\n",
    "\n",
    "gap_fill_volumes = missing_years_in_best_source.merge(serials_with_std_desc, on=['ht_bib_key', 'rights_date_used'], how='inner')\n",
    "gap_fill_volumes = gap_fill_volumes[['htid','ht_bib_key','source', 'description', 'standardized_description','rights_date_used']].drop_duplicates(subset=['ht_bib_key','rights_date_used'])\n",
    "deduplicated_serials = pd.concat([deduplicated_serials, gap_fill_volumes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e708e-d832-401d-966d-450443e71975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 15: Combine all volume groups and export\n",
    "# Merge three groups:\n",
    "#   - unique_volumes: volumes with only one copy (no deduplication needed)\n",
    "#   - nonserial_volumes: duplicated non-serial volumes (one copy per record_id)\n",
    "#   - deduplicated_serials: deduplicated serial volumes (most complete + filled gaps)\n",
    "#\n",
    "# Export final list of deduplicated volumes\n",
    "\n",
    "unique_htids = set(unique_volumes.index.to_list())\n",
    "nonserial_htids = set(nonserial_volumes.htid.to_list())\n",
    "serial_htids = set(deduplicated_serials.htid.to_list())\n",
    "final_deduplicated_volumes = unique_htids.union(nonserial_htids).union(serial_htids)\n",
    "final_deduplicated_volumes = pd.DataFrame(final_deduplicated_volumes, columns=['htid'])\n",
    "\n",
    "# Format HTIDs for HTRC rsync script compatibility\n",
    "final_deduplicated_volumes['clean_htid'] = final_deduplicated_volumes['htid'].apply(lambda x: x.replace(\":\", \"+\").replace(\"/\", \"=\").replace(\"$\",\"\\$\"))\n",
    "final_deduplicated_volumes[['htid']].rename(columns={'htid':'volume'}).to_csv('./deduplicated_volume_list.txt', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1803cb-8499-436f-8a7d-ab4794be3d05",
   "metadata": {},
   "source": [
    "# NEXT STEPS:\n",
    "# 1. Upload 'deduplicated_volume_list.txt' to HTRC Analytics\n",
    "# 2. Use 'Extracted Features Download Helper' to generate rsync script\n",
    "# 3. Run rsync script locally to download .json.bz2 volume files\n",
    "# 4. Proceed to preprocessing and topic modeling stages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
